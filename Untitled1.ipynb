{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e175b5-500a-4e4a-96bc-1cbe63a6b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05182f70-aff6-43be-89da-67bdf9bac262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d5f02-3d4b-4abb-9974-9abb84bab669",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c178cbc-724c-4f30-80e1-140b42556fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
    "    ('scaler', StandardScaler())                 # Standardize numerical features\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b62f9-f2a0-4ff5-b511-8f35819e12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))    # One-hot encode categorical features\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54fd66c-a4a4-41e8-8086-4c34523dfe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2131f-c881-488f-8407-272f89498371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selector', SelectFromModel(RandomForestClassifier())),  # Feature selection\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0dff1558-e34f-4687-961e-012416fb3cb6",
   "metadata": {},
   "source": [
    "8. Interpret the Results and Suggest Improvements\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Accuracy: Measures the proportion of correctly classified instances out of the total instances.\n",
    "Feature Selection: Using a RandomForestClassifier for feature selection helps in identifying the most relevant features automatically.\n",
    "Improvements:\n",
    "\n",
    "Hyperparameter Tuning: Perform grid search or randomized search for hyperparameter tuning to improve model performance.\n",
    "Feature Engineering: Explore additional feature engineering techniques and domain-specific knowledge to enhance the model.\n",
    "Cross-Validation: Implement cross-validation to ensure the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395da73-1bf3-41b1-be76-c23086a2e869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab75a2-07d8-48bf-95cd-34dd013199d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21559f9b-caaf-475b-a534-fc58d3722da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "57c10655-c76f-453f-9770-f76495e71948",
   "metadata": {},
   "source": [
    "QUES 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba1635-bc07-4713-9951-64886f7d7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90680f93-92b0-4684-8b5e-2fc9c0b4aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest classifier pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Create Logistic Regression classifier pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022dad0-0576-421e-a463-257d59f6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a voting classifier combining Random Forest and Logistic Regression\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', rf_pipeline),\n",
    "    ('lr', lr_pipeline)\n",
    "], voting='hard')  # 'hard' voting for majority class prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24d741-744a-4f1c-8aa7-4f04b1c41501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the voting classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Classifier Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc5105cf-8fce-4a79-a9e7-70d2f6e3912b",
   "metadata": {},
   "source": [
    "\n",
    "To address the tasks of building and evaluating machine learning pipelines as described, I'll walk through each step of the process, including code snippets and explanations. This guide will demonstrate how to handle missing values, scale features, perform one-hot encoding, and evaluate models using pipelines and transformers in Python with Scikit-learn.\n",
    "\n",
    "Q1: Building a Pipeline for Feature Engineering and Model Training\n",
    "1. Import Necessary Libraries\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "2. Load and Prepare the Dataset\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Load the dataset\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "3. Identify Numerical and Categorical Features\n",
    "\n",
    "python\n",
    "Copy code\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "4. Create a Numerical Pipeline\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Define numerical pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
    "    ('scaler', StandardScaler())                 # Standardize numerical features\n",
    "])\n",
    "5. Create a Categorical Pipeline\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Define categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))    # One-hot encode categorical features\n",
    "])\n",
    "6. Combine Pipelines Using ColumnTransformer\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Combine numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "7. Create and Train the Random Forest Model\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Create the full pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selector', SelectFromModel(RandomForestClassifier())),  # Feature selection\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "8. Interpret the Results and Suggest Improvements\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Accuracy: Measures the proportion of correctly classified instances out of the total instances.\n",
    "Feature Selection: Using a RandomForestClassifier for feature selection helps in identifying the most relevant features automatically.\n",
    "Improvements:\n",
    "\n",
    "Hyperparameter Tuning: Perform grid search or randomized search for hyperparameter tuning to improve model performance.\n",
    "Feature Engineering: Explore additional feature engineering techniques and domain-specific knowledge to enhance the model.\n",
    "Cross-Validation: Implement cross-validation to ensure the model generalizes well to unseen data.\n",
    "Q2: Building a Pipeline with Random Forest and Logistic Regression Using Voting Classifier\n",
    "1. Import Additional Libraries\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "2. Create the Classifier Pipelines\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Create Random Forest classifier pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Create Logistic Regression classifier pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "3. Create a Voting Classifier\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Create a voting classifier combining Random Forest and Logistic Regression\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', rf_pipeline),\n",
    "    ('lr', lr_pipeline)\n",
    "], voting='hard')  # 'hard' voting for majority class prediction\n",
    "4. Train and Evaluate the Voting Classifier\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the voting classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Classifier Accuracy: {accuracy:.2f}\")\n",
    "5. Interpret the Results and Suggest Improvements\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Accuracy: Evaluates how well the voting classifier combines predictions from Random Forest and Logistic Regression.\n",
    "Voting Strategy: Hard voting uses majority class prediction; soft voting (probability-based) might be explored for better performance.\n",
    "Improvements:\n",
    "\n",
    "Voting Type: Experiment with soft voting to combine predicted probabilities instead of class labels.\n",
    "Hyperparameter Tuning: Optimize hyperparameters for both classifiers to enhance the performance of the voting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c80236-5606-465e-8ba6-f7d1aa3db532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
