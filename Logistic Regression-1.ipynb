{
 "cells": [
  {
   "cell_type": "raw",
   "id": "91beec9d-077b-4ff4-9730-c640fc541bc6",
   "metadata": {},
   "source": [
    "Q1. Difference Between Linear Regression and Logistic Regression\n",
    "Linear Regression:\n",
    "\n",
    "Purpose: Predicts a continuous outcome (e.g., price, temperature).\n",
    "Model: Fits a linear relationship between input features and the target variable.\n",
    "\n",
    "Logistic Regression:\n",
    "\n",
    "Purpose: Predicts a categorical outcome (usually binary, e.g., yes/no, spam/not spam).\n",
    "Model: Models the probability of a binary outcome using the logistic function.\n",
    "\n",
    "Logistic Regression Scenario: Predicting whether an email is spam or not (binary outcome). Linear regression would not be appropriate because it is designed for continuous outcomes, not probabilities or binary classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0a68c-1f49-497f-9692-614b49fba405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2812d423-e3d3-4806-804e-bc3a01277589",
   "metadata": {},
   "source": [
    "Q2. Cost Function in Logistic Regression and Optimization\n",
    "Cost Function:\n",
    "\n",
    "Definition: The cost function (or loss function) in logistic regression is the Binary Cross-Entropy Loss (or Log Loss), which measures how well the predicted probabilities match the actual class labels."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e540875e-f90d-4d59-af24-074a8c22daa4",
   "metadata": {},
   "source": [
    "Q3. Regularization in Logistic Regression\n",
    "Concept:\n",
    "\n",
    "Purpose: Regularization adds a penalty to the cost function to prevent overfitting by discouraging overly complex models.\n",
    "Types:\n",
    "L1 Regularization (Lasso): Adds the absolute value of coefficients to the cost function.\n",
    "Prevents Overfitting: By shrinking the coefficients, regularization helps the model generalize better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ea077-7986-4bbb-a7a3-5a81db0a4d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "d25a4d90-4e70-497e-802c-fcacc5d31990",
   "metadata": {},
   "source": [
    "Q4. ROC Curve and Its Use in Logistic Regression\n",
    "ROC Curve:\n",
    "\n",
    "Definition: The Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n",
    "AUC: The Area Under the ROC Curve (AUC) quantifies the overall performance of the classifier. AUC ranges from 0 to 1, with higher values indicating better performance.\n",
    "Use:\n",
    "\n",
    "Evaluation: Helps evaluate the performance of a logistic regression model, especially in terms of its ability to discriminate between classes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cbd45c9-edd8-4f2e-a06f-a7b981160f35",
   "metadata": {},
   "source": [
    "Q5. Common Techniques for Feature Selection in Logistic Regression\n",
    "Techniques:\n",
    "\n",
    "Backward Elimination: Start with all features and iteratively remove the least significant ones based on p-values or other criteria.\n",
    "Forward Selection: Start with no features and iteratively add features that improve model performance.\n",
    "Regularization: Use L1 (Lasso) regularization to automatically perform feature selection by shrinking some coefficients to zero.\n",
    "Benefits:\n",
    "\n",
    "Improves Model Performance: By reducing the number of irrelevant or redundant features, feature selection can enhance model performance and interpretability."
   ]
  },
  {
   "cell_type": "raw",
   "id": "25778f07-a15f-403b-8cf9-c65ebffef0a9",
   "metadata": {},
   "source": [
    "Q6. Handling Imbalanced Datasets in Logistic Regression\n",
    "Strategies:\n",
    "\n",
    "Resampling:\n",
    "Oversampling: Increase the number of minority class samples (e.g., SMOTE).\n",
    "Undersampling: Decrease the number of majority class samples.\n",
    "Class Weights: Assign higher weights to the minority class in the cost function.\n",
    "Synthetic Data: Generate synthetic examples for the minority class to balance the dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "60df0f95-2501-4e34-89b4-6ce2985a6fa5",
   "metadata": {},
   "source": [
    "Q7. Common Issues and Challenges in Implementing Logistic Regression\n",
    "Issues:\n",
    "\n",
    "Multicollinearity:\n",
    "\n",
    "Problem: High correlation between independent variables can lead to unstable estimates.\n",
    "Solution: Use variance inflation factor (VIF) to detect multicollinearity and consider removing or combining correlated features.\n",
    "Feature Scaling:\n",
    "\n",
    "Problem: Logistic regression may perform poorly if features are on different scales.\n",
    "Solution: Normalize or standardize features before training.\n",
    "Outliers:\n",
    "\n",
    "Problem: Outliers can disproportionately influence the model.\n",
    "Solution: Identify and handle outliers through data preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad773d-3b26-4007-b7bc-e2d54d8084e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
