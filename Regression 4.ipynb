{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b74b747a-098f-40aa-b95b-7ec09b55e962",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and How Does It Differ from Other Regression Techniques?\n",
    "Lasso Regression:\n",
    "\n",
    "Definition: Lasso (Least Absolute Shrinkage and Selection Operator) Regression is a type of linear regression that includes L1 regularization. It adds a penalty proportional to the absolute values of the coefficients to the cost function.\n",
    "Objective Function:\n",
    "\n",
    "RSS: Residual Sum of Squares.\n",
    "λ: Regularization parameter.\n",
    "Differences from Other Regression Techniques:\n",
    "\n",
    "Ordinary Least Squares (OLS): Minimizes only the RSS, with no regularization.\n",
    "Ridge Regression: Uses L2 regularization (penalty proportional to the square of coefficients), which does not produce sparsity in the model.\n",
    "Q2. Main Advantage of Using Lasso Regression in Feature Selection\n",
    "Advantage:\n",
    "\n",
    "Feature Selection: Lasso Regression can set some coefficients exactly to zero, effectively selecting a subset of features. This sparsity is useful for feature selection, reducing the model complexity and improving interpretability.\n",
    "Q3. Interpreting the Coefficients of a Lasso Regression Model\n",
    "Interpretation:\n",
    "\n",
    "Coefficients: In Lasso Regression, non-zero coefficients represent the features that have a significant impact on the response variable. Coefficients that are exactly zero are excluded from the model, indicating that those features are deemed less important for predicting the target variable.\n",
    "Q4. Tuning Parameters in Lasso Regression\n",
    "Tuning Parameter:\n",
    "\n",
    "Regularization Parameter (λ):\n",
    "Effect on Performance:\n",
    "Large λ: Increases the amount of regularization, leading to more coefficients being shrunk towards zero. This may reduce overfitting but can also lead to underfitting.\n",
    "Small λ: Reduces regularization, allowing the model to fit the training data more closely, which may lead to overfitting."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1935ed98-14f2-4d18-b6c6-b3b6e35df908",
   "metadata": {},
   "source": [
    "Q5. Lasso Regression for Non-Linear Problems\n",
    "Usage:\n",
    "\n",
    "Non-Linear Regression: Lasso itself is a linear regression method. To handle non-linear problems, you can use polynomial features or other feature transformations before applying Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9068be6b-0812-456b-81fa-6f130d1e1530",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Polynomial features\u001b[39;00m\n\u001b[1;32m      5\u001b[0m poly \u001b[38;5;241m=\u001b[39m PolynomialFeatures(degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m X_poly \u001b[38;5;241m=\u001b[39m poly\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Lasso Regression with polynomial features\u001b[39;00m\n\u001b[1;32m      9\u001b[0m lasso_model \u001b[38;5;241m=\u001b[39m Lasso(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Polynomial features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Lasso Regression with polynomial features\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_poly, y)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce7bc23f-be01-458b-99b2-01b91eb6f9d6",
   "metadata": {},
   "source": [
    "Q6. Difference Between Ridge Regression and Lasso Regression\n",
    "Ridge Regression:\n",
    "\n",
    "Regularization: L2 (penalty proportional to the square of coefficients).\n",
    "Effect: Shrinks coefficients but does not set any to zero; thus, does not perform feature selection.\n",
    "Lasso Regression:\n",
    "\n",
    "Regularization: L1 (penalty proportional to the absolute values of coefficients).\n",
    "Effect: Can shrink coefficients to zero, effectively performing feature selection.\n",
    "Q7. Handling Multicollinearity with Lasso Regression\n",
    "Effectiveness:\n",
    "\n",
    "Handling Multicollinearity: Lasso Regression helps reduce multicollinearity by shrinking coefficients of correlated features. It can set some coefficients to zero, thus eliminating redundant predictors.\n",
    "Q8. Choosing the Optimal Value of the Regularization Parameter (λ) in Lasso Regression\n",
    "Methods:\n",
    "\n",
    "Cross-Validation: Use k-fold cross-validation to evaluate model performance across different λ values.\n",
    "Grid Search: Test a range of λ values to find the one that minimizes the cross-validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b79ac88-b072-4b16-855c-b7226a66d9de",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ellipsis object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LassoCV\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Example data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Lasso Regression with Cross-Validation\u001b[39;00m\n\u001b[1;32m      7\u001b[0m lasso_cv \u001b[38;5;241m=\u001b[39m LassoCV(alphas\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable ellipsis object"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Example data\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# Lasso Regression with Cross-Validation\n",
    "lasso_cv = LassoCV(alphas=[0.01, 0.1, 1, 10], cv=5)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal λ:\", lasso_cv.alpha_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd1221-6f70-4b2a-ac5a-79144b81ec89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
