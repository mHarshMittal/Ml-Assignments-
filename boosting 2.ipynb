{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e6bb3d07-02fd-4cfe-af52-085051a45abe",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "**Gradient Boosting Regression** is an ensemble technique used to build a predictive model by combining multiple weak learners (typically decision trees) to form a strong model. The primary goal of gradient boosting is to minimize a loss function by iteratively improving the predictions of the model. \n",
    "\n",
    "**Key Features:**\n",
    "- **Boosting**: Sequentially adds models to correct errors made by previous models.\n",
    "- **Gradient Descent**: Optimizes the loss function by gradient descent, where each new model is trained to reduce the residual errors of the combined previous models.\n",
    "- **Regression Task**: Specifically focuses on predicting continuous target variables.\n",
    "\n",
    "### Q2. Implement a Simple Gradient Boosting Algorithm from Scratch Using Python and NumPy\n",
    "\n",
    "Here’s a basic implementation of gradient boosting regression from scratch using Python and NumPy. \n",
    "\n",
    "#### Step-by-Step Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a simple regression dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "# Define a function to fit gradient boosting regression\n",
    "def gradient_boosting(X, y, n_estimators=10, learning_rate=0.1, max_depth=3):\n",
    "    # Initialize the model with a simple prediction (mean of y)\n",
    "    y_pred = np.mean(y) * np.ones_like(y)\n",
    "    models = []\n",
    "    \n",
    "    for _ in range(n_estimators):\n",
    "        # Compute the residuals\n",
    "        residuals = y - y_pred\n",
    "        \n",
    "        # Fit a decision tree regressor on residuals\n",
    "        model = DecisionTreeRegressor(max_depth=max_depth)\n",
    "        model.fit(X, residuals)\n",
    "        models.append(model)\n",
    "        \n",
    "        # Update predictions with the new model\n",
    "        y_pred += learning_rate * model.predict(X)\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Train the gradient boosting model\n",
    "n_estimators = 50\n",
    "learning_rate = 0.1\n",
    "models = gradient_boosting(X, y, n_estimators, learning_rate)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = np.mean(y) * np.ones_like(y)\n",
    "for model in models:\n",
    "    y_pred += learning_rate * model.predict(X)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "```\n",
    "\n",
    "#### Explanation:\n",
    "1. **Dataset Generation**: We create a synthetic dataset where \\( y = 3x + \\text{noise} \\).\n",
    "2. **Model Initialization**: Start with an initial prediction as the mean of the target values.\n",
    "3. **Residual Calculation**: Compute residuals, which are the differences between the true values and current predictions.\n",
    "4. **Fit Decision Tree**: Fit a decision tree on the residuals to learn the residual patterns.\n",
    "5. **Update Predictions**: Adjust predictions by adding the weighted predictions from the new model.\n",
    "6. **Evaluation**: Compute the Mean Squared Error (MSE) and R-squared values to evaluate performance.\n",
    "\n",
    "### Q3. Experiment with Different Hyperparameters\n",
    "\n",
    "To optimize the performance of the gradient boosting model, you can experiment with different hyperparameters like learning rate, number of trees, and tree depth. Using `GridSearchCV` or `RandomizedSearchCV` from `scikit-learn` can help in finding the best combination of hyperparameters.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Initialize the GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Set up the GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best parameters\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f'Optimized Mean Squared Error: {mse}')\n",
    "print(f'Optimized R-squared: {r2}')\n",
    "```\n",
    "\n",
    "### Q4. What is a Weak Learner in Gradient Boosting?\n",
    "\n",
    "A **weak learner** is a model that performs slightly better than random guessing. In the context of gradient boosting, weak learners are typically simple models, such as shallow decision trees (also known as \"stumps\"). The purpose of weak learners is to provide a base model that can be improved iteratively by focusing on the errors of previous models.\n",
    "\n",
    "### Q5. What is the Intuition Behind the Gradient Boosting Algorithm?\n",
    "\n",
    "The intuition behind gradient boosting is to build a strong predictive model by sequentially adding weak learners. Each weak learner corrects the errors of its predecessors by focusing on the residuals, which are the differences between the actual values and the predicted values of the current ensemble. Gradient descent is used to minimize the loss function by updating the model's predictions iteratively.\n",
    "\n",
    "### Q6. How Does Gradient Boosting Algorithm Build an Ensemble of Weak Learners?\n",
    "\n",
    "**Building an Ensemble**:\n",
    "1. **Initialize**: Start with an initial model, often a simple model like the mean of the target values.\n",
    "2. **Fit Weak Learners**: Train each weak learner on the residual errors of the combined previous models.\n",
    "3. **Update Predictions**: Add the new weak learner's predictions, scaled by a learning rate, to the current predictions.\n",
    "4. **Iterate**: Repeat the process to build an ensemble of weak learners that collectively improve the model’s accuracy.\n",
    "\n",
    "### Q7. What are the Steps Involved in Constructing the Mathematical Intuition of Gradient Boosting Algorithm?\n",
    "\n",
    "1. **Initialization**: Start with an initial prediction, usually the mean of the target values or a constant.\n",
    "2. **Compute Residuals**: Calculate the residuals, which are the differences between the true values and the current predictions.\n",
    "3. **Fit Weak Learner**: Train a weak learner (e.g., a small decision tree) on the residuals to model the errors.\n",
    "4. **Update Model**: Adjust the model's predictions by adding the weak learner’s predictions, scaled by a learning rate.\n",
    "5. **Iterate**: Repeat the process for a specified number of iterations or until the model converges.\n",
    "6. **Combine**: Aggregate the predictions from all weak learners to form the final strong predictive model.\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "\n",
    "1. **Initial Model**: \\( f_0(x) = \\text{mean}(y) \\)\n",
    "2. **Residual Calculation**: \\( r_i = y_i - f_{m-1}(x_i) \\)\n",
    "3. **Weak Learner Fit**: \\( h_m(x) \\) fitted on residuals.\n",
    "4. **Update Model**: \\( f_m(x) = f_{m-1}(x) + \\alpha_m \\cdot h_m(x) \\)\n",
    "   - \\( \\alpha_m \\) is the learning rate.\n",
    "\n",
    "By iteratively applying these steps, gradient boosting builds a robust model that captures complex patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
