{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a65658c3-fa5c-47ac-893a-05127d07da92",
   "metadata": {},
   "source": [
    "### Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example.\n",
    "\n",
    "**Eigenvalues** and **eigenvectors** are fundamental concepts in linear algebra used to understand linear transformations of vectors.\n",
    "\n",
    "- **Eigenvalue (λ)**: Given a square matrix \\( A \\), an eigenvalue \\( λ \\) is a scalar that satisfies the equation \\( A \\mathbf{v} = λ \\mathbf{v} \\), where \\( \\mathbf{v} \\) is a non-zero vector. It represents how much the eigenvector is scaled during the transformation by \\( A \\).\n",
    "  \n",
    "- **Eigenvector (v)**: An eigenvector \\( \\mathbf{v} \\) of a square matrix \\( A \\) is a non-zero vector that, when multiplied by \\( A \\), results in a scalar multiple of itself. It satisfies \\( A \\mathbf{v} = λ \\mathbf{v} \\). It represents the direction along which the transformation \\( A \\) acts by simply scaling.\n",
    "\n",
    "**Eigen-Decomposition**: This is a way to factorize a matrix \\( A \\) into the product of its eigenvectors and eigenvalues. For a matrix \\( A \\) with eigenvalues \\( λ_i \\) and corresponding eigenvectors \\( \\mathbf{v}_i \\), the matrix can be decomposed as:\n",
    "\\[ A = V \\Lambda V^{-1} \\]\n",
    "where \\( V \\) is the matrix of eigenvectors and \\( \\Lambda \\) is a diagonal matrix with eigenvalues \\( λ_i \\) on its diagonal.\n",
    "\n",
    "**Example**:\n",
    "Consider the matrix:\n",
    "\\[ A = \\begin{pmatrix} 4 & 1 \\\\ 2 & 3 \\end{pmatrix} \\]\n",
    "\n",
    "To find eigenvalues and eigenvectors:\n",
    "1. Solve the characteristic equation \\( \\text{det}(A - λI) = 0 \\):\n",
    "   \\[ \\text{det}\\left(\\begin{pmatrix} 4-λ & 1 \\\\ 2 & 3-λ \\end{pmatrix}\\right) = (4-λ)(3-λ) - 2 \\cdot 1 = λ^2 - 7λ + 10 = 0 \\]\n",
    "   The eigenvalues are \\( λ_1 = 5 \\) and \\( λ_2 = 2 \\).\n",
    "\n",
    "2. For \\( λ_1 = 5 \\), solve \\( (A - 5I) \\mathbf{v} = 0 \\):\n",
    "   \\[ \\begin{pmatrix} -1 & 1 \\\\ 2 & -2 \\end{pmatrix} \\mathbf{v} = 0 \\]\n",
    "   Eigenvector \\( \\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\).\n",
    "\n",
    "3. For \\( λ_2 = 2 \\), solve \\( (A - 2I) \\mathbf{v} = 0 \\):\n",
    "   \\[ \\begin{pmatrix} 2 & 1 \\\\ 2 & 1 \\end{pmatrix} \\mathbf{v} = 0 \\]\n",
    "   Eigenvector \\( \\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} \\).\n",
    "\n",
    "### Q2. What is eigen decomposition and what is its significance in linear algebra?\n",
    "\n",
    "**Eigen Decomposition** is the factorization of a square matrix \\( A \\) into:\n",
    "\\[ A = V \\Lambda V^{-1} \\]\n",
    "where:\n",
    "- \\( V \\) is the matrix whose columns are the eigenvectors of \\( A \\).\n",
    "- \\( \\Lambda \\) is a diagonal matrix with the corresponding eigenvalues of \\( A \\) on the diagonal.\n",
    "- \\( V^{-1} \\) is the inverse of matrix \\( V \\).\n",
    "\n",
    "**Significance**:\n",
    "- **Diagonalization**: Eigen decomposition simplifies matrix operations, such as matrix powers and exponentials, by transforming the matrix into a diagonal form.\n",
    "- **Understanding Transformations**: It provides insight into how the matrix transforms vectors, highlighting directions of maximum variance or stretching.\n",
    "- **Applications**: Used in solving differential equations, stability analysis, and various numerical methods.\n",
    "\n",
    "### Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer.\n",
    "\n",
    "For a square matrix \\( A \\) to be diagonalizable, it must satisfy the following conditions:\n",
    "\n",
    "1. **Existence of Eigenvalues**: The matrix \\( A \\) must have \\( n \\) eigenvalues (where \\( n \\) is the size of the matrix), which could be real or complex.\n",
    "2. **Linearly Independent Eigenvectors**: The matrix must have \\( n \\) linearly independent eigenvectors. In other words, there must be a complete basis of eigenvectors for the space.\n",
    "\n",
    "**Proof Outline**:\n",
    "- **Existence**: A matrix of size \\( n \\times n \\) has at most \\( n \\) eigenvalues, counting multiplicities.\n",
    "- **Independence**: If a matrix \\( A \\) has \\( n \\) distinct eigenvalues, it always has \\( n \\) linearly independent eigenvectors. For matrices with repeated eigenvalues, \\( A \\) is diagonalizable if and only if the total number of linearly independent eigenvectors equals \\( n \\).\n",
    "\n",
    "### Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example.\n",
    "\n",
    "**Spectral Theorem**: In linear algebra, the spectral theorem states that any symmetric matrix \\( A \\) can be diagonalized by an orthogonal matrix. In other words, if \\( A \\) is symmetric, there exists an orthogonal matrix \\( Q \\) and a diagonal matrix \\( \\Lambda \\) such that:\n",
    "\\[ A = Q \\Lambda Q^T \\]\n",
    "\n",
    "**Significance**:\n",
    "- **Diagonalization**: The theorem guarantees that symmetric matrices can be diagonalized using orthogonal matrices, simplifying many matrix operations.\n",
    "- **Orthogonality**: The eigenvectors are orthogonal, which simplifies computations and ensures numerical stability.\n",
    "\n",
    "**Example**:\n",
    "Consider the symmetric matrix:\n",
    "\\[ A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\]\n",
    "\n",
    "The eigenvalues are \\( 3 \\) and \\( 1 \\), and the corresponding orthonormal eigenvectors are \\( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\) and \\( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\). \n",
    "\n",
    "Thus:\n",
    "\\[ A = Q \\Lambda Q^T \\]\n",
    "where:\n",
    "\\[ Q = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\end{pmatrix}, \\quad \\Lambda = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} \\]\n",
    "\n",
    "### Q5. How do you find the eigenvalues of a matrix and what do they represent?\n",
    "\n",
    "To find the eigenvalues of a matrix \\( A \\), solve the characteristic polynomial equation:\n",
    "\\[ \\text{det}(A - λI) = 0 \\]\n",
    "where \\( I \\) is the identity matrix of the same size as \\( A \\) and \\( λ \\) is the eigenvalue.\n",
    "\n",
    "**Steps**:\n",
    "1. **Form Matrix**: Subtract \\( λ \\) times the identity matrix from \\( A \\).\n",
    "2. **Compute Determinant**: Find the determinant of the resulting matrix.\n",
    "3. **Solve Polynomial**: Set the determinant to zero and solve for \\( λ \\).\n",
    "\n",
    "**Representation**:\n",
    "- **Scaling Factor**: Eigenvalues represent how much the corresponding eigenvectors are scaled during the linear transformation described by matrix \\( A \\).\n",
    "- **Principal Components**: In PCA, eigenvalues indicate the amount of variance captured by each principal component.\n",
    "\n",
    "### Q6. What are eigenvectors and how are they related to eigenvalues?\n",
    "\n",
    "**Eigenvectors** are non-zero vectors that, when multiplied by a matrix \\( A \\), result in a scaled version of the same vector. Formally, if \\( A \\mathbf{v} = λ \\mathbf{v} \\), then \\( \\mathbf{v} \\) is an eigenvector corresponding to the eigenvalue \\( λ \\).\n",
    "\n",
    "**Relation**:\n",
    "- **Scaling**: Eigenvectors represent directions along which the matrix \\( A \\) acts by scaling (via eigenvalues) rather than rotating or otherwise transforming.\n",
    "- **Principal Directions**: In various applications, eigenvectors often represent principal directions or modes of the data.\n",
    "\n",
    "### Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?\n",
    "\n",
    "**Geometric Interpretation**:\n",
    "- **Eigenvectors**: Represent directions in the space that remain unchanged except for scaling when a matrix transformation is applied.\n",
    "- **Eigenvalues**: Represent the scaling factor along the direction of the eigenvectors.\n",
    "\n",
    "**Visualization**:\n",
    "- If a matrix \\( A \\) represents a linear transformation, applying \\( A \\) to an eigenvector \\( \\mathbf{v} \\) stretches or compresses \\( \\mathbf{v} \\) by the factor \\( λ \\), but does not change its direction.\n",
    "\n",
    "### Q8. What are some real-world applications of eigen decomposition?\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**: Reduces the dimensionality of data by projecting it onto principal components, which are eigenvectors of the covariance matrix.\n",
    "2. **Face Recognition**: Eigenfaces method uses eigen decomposition of the covariance matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
