{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fd0f3e09-da39-47b2-882e-38560463a504",
   "metadata": {},
   "source": [
    "Q1. Purpose of Grid Search CV in Machine Learning\n",
    "Grid Search CV (Cross-Validation):\n",
    "\n",
    "Purpose: To find the optimal hyperparameters for a machine learning model by exhaustively searching through a specified set of parameter values.\n",
    "How It Works:\n",
    "Define Parameter Grid: Specify the hyperparameters and their possible values.\n",
    "Cross-Validation: For each combination of parameters, perform k-fold cross-validation to evaluate model performance.\n",
    "Select Best Parameters: Choose the combination that results in the best performance metric (e.g., accuracy, F1 score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58df504-71ce-482b-a941-cadfc0818709",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Fit grid search to data\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# Initialize model and GridSearchCV\n",
    "model = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit grid search to data\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bdbe4c1-4f63-473e-a9d4-f04caad25c24",
   "metadata": {},
   "source": [
    "\n",
    "Here are concise explanations and examples for your questions on grid search, data leakage, and confusion matrices:\n",
    "\n",
    "Q1. Purpose of Grid Search CV in Machine Learning\n",
    "Grid Search CV (Cross-Validation):\n",
    "\n",
    "Purpose: To find the optimal hyperparameters for a machine learning model by exhaustively searching through a specified set of parameter values.\n",
    "How It Works:\n",
    "Define Parameter Grid: Specify the hyperparameters and their possible values.\n",
    "Cross-Validation: For each combination of parameters, perform k-fold cross-validation to evaluate model performance.\n",
    "Select Best Parameters: Choose the combination that results in the best performance metric (e.g., accuracy, F1 score).\n",
    "Python Code Example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# Initialize model and GridSearchCV\n",
    "model = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit grid search to data\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "Q2. Difference Between Grid Search CV and Randomized Search CV\n",
    "Grid Search CV:\n",
    "\n",
    "Definition: Systematically searches through all possible combinations of specified hyperparameters.\n",
    "Pros: Thorough, guaranteed to find the best combination within the specified grid.\n",
    "Cons: Computationally expensive, especially with large grids.\n",
    "Randomized Search CV:\n",
    "\n",
    "Definition: Randomly samples a fixed number of parameter combinations from a defined distribution.\n",
    "Pros: More efficient with large hyperparameter spaces, less computationally intensive.\n",
    "Cons: May miss the optimal combination, as it does not explore all possibilities.\n",
    "When to Choose:\n",
    "\n",
    "Grid Search CV: When you have a manageable parameter space and need an exhaustive search.\n",
    "Randomized Search CV: When dealing with large hyperparameter spaces or computational constraints.\n",
    "Q3. Data Leakage and Its Problems\n",
    "Data Leakage:\n",
    "\n",
    "Definition: Occurs when information from outside the training dataset is used to create the model, leading to over-optimistic performance estimates.\n",
    "Why It’s a Problem: Leads to overly optimistic performance metrics and poor generalization to new, unseen data.\n",
    "Example:\n",
    "\n",
    "Example: Including the test data in the training set or using future information to predict past events.\n",
    "Q4. Preventing Data Leakage\n",
    "Prevention Strategies:\n",
    "\n",
    "Separate Data Splits: Ensure that the training, validation, and test sets are strictly separated.\n",
    "Feature Engineering: Perform feature engineering only on the training data and apply transformations to the test data afterward.\n",
    "Cross-Validation: Use proper cross-validation techniques to ensure that data leakage does not occur during the validation process."
   ]
  },
  {
   "cell_type": "raw",
   "id": "92e88080-c289-4dfd-8159-3c5149f86983",
   "metadata": {},
   "source": [
    "Q5. Confusion Matrix\n",
    "Definition: A confusion matrix is a table used to evaluate the performance of a classification model by summarizing the true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "Components:\n",
    "\n",
    "True Positives (TP): Correctly predicted positive cases.\n",
    "True Negatives (TN): Correctly predicted negative cases.\n",
    "False Positives (FP): Incorrectly predicted positive cases.\n",
    "False Negatives (FN): Incorrectly predicted negative cases."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ee07100-fad4-45ef-8f93-dd8d1568a9e1",
   "metadata": {},
   "source": [
    "Q6. Difference Between Precision and Recall\n",
    "Precision:\n",
    "\n",
    "Definition: The ratio of true positives to the total number of predicted positives.\n",
    "\n",
    "Definition: The ratio of true positives to the total number of actual positives.\n",
    "\n",
    "Precision: Important when the cost of false positives is high.\n",
    "Recall: Important when the cost of false negatives is high."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c973cad0-4505-41be-8ca1-254af64b3c1b",
   "metadata": {},
   "source": [
    "Q7. Interpreting a Confusion Matrix\n",
    "Types of Errors:\n",
    "\n",
    "False Positives (FP): Type I Error – Model incorrectly predicts positive when it is negative.\n",
    "False Negatives (FN): Type II Error – Model incorrectly predicts negative when it is positive.\n",
    "Interpretation:\n",
    "\n",
    "Analyze FP and FN to understand where the model is making errors. Adjust model thresholds or use different metrics (e.g., F1 score) based on the type of error that is more costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1932c8d2-e64d-46af-8a4d-1625baeaa60f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43my_test\u001b[49m, y_pred)\n\u001b[1;32m      5\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y_test, y_pred)\n\u001b[1;32m      6\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(y_test, y_pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c4625b9-9679-49ef-9c00-06854a2a8ea7",
   "metadata": {},
   "source": [
    "Q9. Relationship Between Accuracy and Confusion Matrix Values\n",
    "Accuracy:\n",
    "\n",
    "Definition: The ratio of correctly predicted observations to the total observations\n",
    "Accuracy provides an overall measure of the model's performance but may be misleading in imbalanced datasets where the number of instances in each class is unequal"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c789d01-082d-4b7e-b364-c92587b204bb",
   "metadata": {},
   "source": [
    "Q10. Using a Confusion Matrix to Identify Biases or Limitations\n",
    "Biases and Limitations:\n",
    "\n",
    "Class Imbalance: If the model performs well in terms of accuracy but has poor precision or recall for the minority class, it may indicate bias towards the majority class.\n",
    "Threshold Adjustment: Analyze the confusion matrix to adjust classification thresholds to balance precision and recall.\n",
    "Example:\n",
    "\n",
    "Imbalanced Dataset: High accuracy but poor performance in detecting the minority class may necessitate using metrics like F1 score or adjusting class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86d102-fa4c-448d-b097-4bc3e0fe74a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
