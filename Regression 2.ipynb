{
 "cells": [
  {
   "cell_type": "raw",
   "id": "55498686-57b2-4cf6-a280-02e405979743",
   "metadata": {},
   "source": [
    "Q1. Concept of R-squared\n",
    "Definition:\n",
    "\n",
    "R-squared (R²) is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "Interpretation:\n",
    "\n",
    "R² ranges from 0 to 1. A value of 1 indicates that the model explains all the variability of the response data around its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d86af-c780-413e-bd6d-de81c2dc263a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "cfc1e4c3-b5bc-40bc-8694-ebcb986e02b9",
   "metadata": {},
   "source": [
    "Q2. Adjusted R-squared\n",
    "Definition:\n",
    "\n",
    "Adjusted R-squared adjusts the R-squared value for the number of predictors in the model.\n",
    "Difference from R-squared:\n",
    "\n",
    "R-squared always increases with more predictors, regardless of their relevance.\n",
    "Adjusted R-squared accounts for the number of predictors and can decrease if the predictors do not improve the model significantly."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3aecdcf3-806d-428e-8b94-15d6dbaaf8c0",
   "metadata": {},
   "source": [
    "Q3. When to Use Adjusted R-squared\n",
    "Appropriate Use:\n",
    "\n",
    "Use Adjusted R-squared when comparing models with different numbers of predictors.\n",
    "It provides a more accurate measure of model performance by penalizing the addition of irrelevant predictors."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2aba7f9-2fcd-46a6-ae83-8efb5237bb98",
   "metadata": {},
   "source": [
    "MSE: Penalizes larger errors more than smaller errors.\n",
    "RMSE: Interpreted in the same unit as the target variable.\n",
    "MAE: More robust to outliers compared to MSE."
   ]
  },
  {
   "cell_type": "raw",
   "id": "daee9e80-6756-4a96-afa2-e96352490d4f",
   "metadata": {},
   "source": [
    "Q5. Advantages and Disadvantages of RMSE, MSE, and MAE\n",
    "Advantages:\n",
    "\n",
    "MSE: Sensitive to outliers, useful when large errors are particularly undesirable.\n",
    "RMSE: Provides error measurement in the same unit as the response variable.\n",
    "MAE: Provides a clear understanding of average error magnitude, less sensitive to outliers."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca8bf53e-8c92-47f2-990f-7b7be1a1ff78",
   "metadata": {},
   "source": [
    "Q6. Lasso Regularization\n",
    "Definition:\n",
    "   \n",
    "Lasso Regularization (L1 regularization) adds a penalty proportional to the absolute value of the coefficients to the cost function.\n",
    "Difference from Ridge Regularization:\n",
    "\n",
    "Lasso: Can lead to sparsity (some coefficients become zero), useful for feature selection.\n",
    "Ridge: Adds a penalty proportional to the square of the coefficients (L2 regularization), usually retains all features but with smaller coefficients.\n",
    "Appropriate Use:\n",
    "\n",
    "Use Lasso when you suspect that only a subset of features is important or when feature selection is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7bbdcb-7924-4e2b-8d00-6f7fd5b7e6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c0521a-5391-470e-9265-010ebd1182a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ellipsis object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Ridge Regression\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ridge_model \u001b[38;5;241m=\u001b[39m Ridge(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable ellipsis object"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example data\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "ridge_predictions = ridge_model.predict(X_test)\n",
    "print(\"Ridge RMSE:\", np.sqrt(mean_squared_error(y_test, ridge_predictions)))\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "lasso_predictions = lasso_model.predict(X_test)\n",
    "print(\"Lasso RMSE:\", np.sqrt(mean_squared_error(y_test, lasso_predictions)))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce466139-f8e0-44f1-8007-d467b28c5c0f",
   "metadata": {},
   "source": [
    "Q8. Limitations of Regularized Linear Models\n",
    "Limitations:\n",
    "\n",
    "Over-Simplification: Regularization can overly simplify the model, potentially missing important patterns.\n",
    "Choice of Parameter: Selecting the optimal regularization parameter (λ) is crucial and can be challenging.\n",
    "When They Might Not Be Ideal:\n",
    "\n",
    "When the true relationship between predictors and response is complex and requires capturing intricate patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32e752-0288-4388-92b6-6c839a49c124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "dad62a52-6c66-4510-be74-b9dea3aafd49",
   "metadata": {},
   "source": [
    "Q9. Comparing Models Based on RMSE and MAE\n",
    "Comparison:\n",
    "\n",
    "Model A (RMSE = 10): Provides information about the error in the same units as the response but is more sensitive to outliers.\n",
    "Model B (MAE = 8): Provides an average magnitude of errors and is less sensitive to outliers.\n",
    "Choice:\n",
    "\n",
    "Choose based on context. If large errors are particularly problematic, prefer RMSE. For a more robust measure against outliers, prefer MAE.\n",
    "Limitations:\n",
    "\n",
    "RMSE and MAE can provide different perspectives on model performance. A lower RMSE doesn't always mean better overall performance if outliers are a concern."
   ]
  },
  {
   "cell_type": "raw",
   "id": "571f62d8-d294-42d2-a5cd-1c5d4d1e3ac1",
   "metadata": {},
   "source": [
    "Q10. Comparing Ridge and Lasso Regularization\n",
    "Comparison:\n",
    "\n",
    "Ridge Regularization (α = 0.1): Penalizes the square of coefficients, retains all features but with smaller coefficients.\n",
    "Lasso Regularization (α = 0.5): Can zero out some coefficients, useful for feature selection.\n",
    "Choice:\n",
    "\n",
    "Use Ridge if you believe all features are relevant but might need smaller coefficients.\n",
    "Use Lasso if you suspect some features are not useful and want automatic feature selection.\n",
    "Trade-offs:\n",
    "\n",
    "Lasso may lead to sparsity but may remove important features.\n",
    "Ridge provides a solution with all features included but with reduced impact.\n",
    "Feel free to copy these explanations and code snippets into your Jupyter Notebook. Let me know if you need more details or additional questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16515c65-2a3f-4297-9e48-d4e9c015ef12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
